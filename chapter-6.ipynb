{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chapter 6 - Branching processes\n",
    "\n",
    "Thinking about the problem where there is a probability that a man has 0, 1, 2,..., children with $p_0$, $p_1$ etc., we can represent a branching process. A simple, discrete, branching process is teh Galton-Watson process. This process requires:\n",
    "1. that we start with a single individual $N_0 = 1$\n",
    "2. that the number of offspring sired by that individual be a discrete random variable of given distribution: $$P(N = n)  = p_n, \\text{with} \\ \\ \\sum^{\\infty}_{n=0} p_n = 1$$and\n",
    "3. that the conditional distribution of $N_{t+1}$, given $N_t = n$ is the sum of $n$ independent variables, each with the same distrubtion as $N_1$. \n",
    "\n",
    "The third condition makes the Galton-Watson process a Markov chain. To look at these problems, we'll again use the probability generating function $$F(x) = E(x^{N_1}) = \\sum_{n=0}^{\\infty} p_n x^n.$$\n",
    "\n",
    "There is a theorem here to note. Let $F$ & $G$ be two random variables with probability mass functions $f_k$ and $g_k$ and generating functions $F(x)$ and $G(x)$. For the new random variable $H \\equiv F + G$ with probability mass function $h_k$, the probability generating function $H(x)$ is given by \n",
    "\n",
    "$$H(x) = F(x)G(x)$$ \n",
    "\n",
    "Expanding this, the probability generating function for the sum of $n$ variables is simply the corresponding n-fold product of each individual generating function. \n",
    "\n",
    "A second theorem: Let $S_N$ be the sum of N independent, identically distributed, random variables with the common probability generating function $G(x)$. If $N (\\geq 0)$ is an independent random variable with probability generating function $F(x)$ (so that we are taking the sum of a random number of identical random variables), the probability generating function $H(x)$ of $S_N$ is given by: \n",
    "\n",
    "$$H(x) = F(G(x))$$.\n",
    "\n",
    "Instead of computing each probability of the G-W process individually, we can determine the probability generating function $F_t(x)$ for each generation $t$. By our first requirements, we have $$F_0(x) = x$$ This simply reiterates that we're starting with an individual. Following the second requirement, the probability generating function for the first generation is, as a result, given as: $$F_1(x) = F(x)$$. For the next generation, we now have a random number of individuals. By our second theorem, we get \n",
    "\n",
    "$$F_2(x) = F[F1(x)] = F \\circ F(x).$$ \n",
    "\n",
    "With each new generation, we iterate with the generating function $F(x)$, \n",
    "\n",
    "$$F_t(x) = F[F_{t-1}(x)] = F^t(x),$$\n",
    "\n",
    "to obtain a t-fold composition in generation t. We can find the expected value of $N_t$ by differentiating the above equation at $x=1:$ \n",
    "\n",
    "$$E(N_t) = \\frac{dF_t}{dx}\\biggr\\rvert_{x = 1}$$\n",
    "\n",
    "and we determine after some maths, that $E(N_t) = R_0^t$ so that the population grows on average, geometrically, with a net reproductive rate equal to the mean number of offspring. We can also find the fixed point (a challenge), and the variance (simple, from chapter 3). \n",
    "\n",
    "The probability of being extinct in generation $t$ (the original question), if $F_t(0)$. So, \n",
    "\n",
    "$$F_t(0) = F[F_{t-1}(0)] = F^t(0).$$\n",
    "\n",
    "We can get the chance of being extinct in any generation by recurrsively iterating the probability generating function with $x = 0$. However, this is the same as cobwebbing along probability generating function $F(x) with initial condition zero.\n",
    "\n",
    "If we think about the case $0 < p_0 < 1$, the probability generating function may take three different forms depending on the size of $R_0 = F'(1).$ If $R_0 = F'(1) < 1$ then extinction will occur with probability 1 and we could look at the conditional distribution for the population size, conditional on no extinction. After one generation, the generating function for this condition distribution is \n",
    "\n",
    "$$ G(x) = \\sum^{\\infty}_{n=1} \\frac{p_n}{1 - p_0}x^n.$$\n",
    "\n",
    "This can be manipulated and shown to be dependent on $F(x)$, and for large $t$, the limit of $G_t(x)$, $G^*(x)$, must thus satisfy the functional equation\n",
    "\n",
    "$$ G^*[F(x)] = 1 + [G^*(x)-1]R_0.$$\n",
    "\n",
    "If a population with a subcritical net reproductive rate has not gone extinct after some long period, it is probably in a 'stable'statisticaly quasistationary state given by this limiting distribution. \n",
    "\n",
    "If $R_0 = 1$, then extinction again is probability = 1, but it's very slow. \n",
    "\n",
    "The final alternative is if $R_0 = F'(1) > 1$ (the supercritical case). Here, $$F(x) = x$$ has a unique solution in the open interval (0,1). This fixed point if asymptotically stable. The abscissa (x-value) is the asmptotic probability of extinction. \n",
    "\n",
    "If a population does not die out in this case, it diverges to infinity. To describe this growth, consider the random variable consisting of the population size in the i-th generation normed by the expected population size in the t-th generation: \n",
    "\n",
    "$$W_t \\equiv \\frac{N_t}{R_0^t}$$\n",
    "\n",
    "This random variable has some interesting properties. First,\n",
    "\n",
    "$$E(W_t) = \\frac{1}{R_0^t}E(N_t) = 1.$$\n",
    "\n",
    "Secondly, \n",
    "\n",
    "$$Var(W_t) = \\frac{1}{R_0^{2t}Var(N_t)},$$\n",
    "\n",
    "and most importantly, \n",
    "\n",
    "$$E(N_{t+1}|N_t) = R_0N_t,$$\n",
    "\n",
    "which by the Markov property, can be rewritten\n",
    "\n",
    "$$ E(N_{n+1}|N_1,N_2,..,N_t) = R_0N_t.$$\n",
    "\n",
    "We can rewrite this in terms of $W_t$:\n",
    "\n",
    "$$E(W_{t+1}|N_1,N_2,...,N_t) = W_t$$. \n",
    "\n",
    "This last result implies that $W_t$ is a discrete parameter *martingale*. \n",
    "\n",
    "**Martingale:** A sequence ${W_t:t\\geq1}$ is a *martingale* with respect to the sequence ${N_t:t\\geq1}$ if, for all $t \\geq 1:$\n",
    "\n",
    "1. $E(|W_t|) < \\infty$\n",
    "2. $E(W_{t+1}|N_1,N_2,...N_t) = W_t.$\n",
    "\n",
    "Martingales are important because, subject to minor conditions on the moments of $W_t$, they always converge. This is the 'martingale convergence theorem' of Doob. \n",
    "\n",
    "The limiting distribution of $W_t$ is known explicitly in only a few cases. For fractional linear generating functions, the limiting distribution $W$ is an exponential distribution. In general, the Laplace transform $\\phi(s)$ of the limiting distribution $W$ satisfies PoincarÃ©'s functional equation \n",
    "\n",
    "$$ \\phi(s) = F[\\phi(s/R_0)].$$\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
